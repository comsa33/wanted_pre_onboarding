{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "이루오_wanted.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qIbM8jGUNdBu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.word_dict = {'oov': 0}\n",
        "        self.fit_checker = False\n",
        "  \n",
        "    def preprocessing(self, sequences):\n",
        "        result = []\n",
        "        for sent in sequences:\n",
        "            sent = re.sub(r'[^ ㄱ-ㅣ가-힣A-Za-z0-9]', '', sent)\n",
        "            result.append(list(map(lambda x: x.lower(), sent.split())))\n",
        "        return result\n",
        "  \n",
        "    def fit(self, sequences):\n",
        "        self.fit_checker = False\n",
        "        token = self.preprocessing(sequences)\n",
        "        for sent in token:\n",
        "            for word in sent:\n",
        "                self.word_dict.setdefault(word, len(self.word_dict))\n",
        "        self.fit_checker = True\n",
        "  \n",
        "    def transform(self, sequences):\n",
        "        result = []\n",
        "        tokens = self.preprocessing(sequences)\n",
        "        if self.fit_checker:\n",
        "            for sent in tokens:\n",
        "                temp = []\n",
        "                for word in sent:\n",
        "                    temp.append(self.word_dict[word])\n",
        "                result.append(temp)\n",
        "            return result\n",
        "        else:\n",
        "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        result = self.transform(sequences)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class TfidfVectorizer:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.fit_checker = False\n",
        "  \n",
        "    def fit(self, sequences):\n",
        "        tokenized = self.tokenizer.fit_transform(sequences)\n",
        "        \n",
        "        self.idf = []\n",
        "        n = len(tokenized)\n",
        "        tokenized_flatten = set([y for x in tokenized for y in x])\n",
        "        \n",
        "        for i in tokenized_flatten:\n",
        "            df = np.sum([1 if i in token else 0 for token in tokenized])\n",
        "            idf = np.log(n/1+df)\n",
        "            self.idf.append(idf)\n",
        "        self.idf = np.array(self.idf).T\n",
        "\n",
        "        self.fit_checker = True\n",
        "    \n",
        "\n",
        "    def transform(self, sequences):\n",
        "        if self.fit_checker:\n",
        "            tokenized = self.tokenizer.transform(sequences)\n",
        "\n",
        "            tokenized_flatten = set([y for x in tokenized for y in x])\n",
        "            tf = []\n",
        "            for i in tokenized_flatten:\n",
        "                dtm = [token.count(i) for token in tokenized]\n",
        "                tf.append(dtm)\n",
        "                tf_ = np.array(tf).T\n",
        "            self.tfidf_matrix = tf_ * self.idf\n",
        "\n",
        "            return self.tfidf_matrix\n",
        "        else:\n",
        "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        return self.transform(sequences)"
      ],
      "metadata": {
        "id": "72BWOMKbOOET"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}