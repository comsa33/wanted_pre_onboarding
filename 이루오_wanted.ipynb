{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "이루오_wanted.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qIbM8jGUNdBu"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.word_dict = {'oov': 0}\n",
        "        self.fit_checker = False\n",
        "  \n",
        "    def preprocessing(self, sequences):\n",
        "        import re\n",
        "        result = []\n",
        "        for sent in sequences:\n",
        "            sent = re.sub(r'[^ ㄱ-ㅣ가-힣A-Za-z0-9]', '', sent)\n",
        "            result.append(list(map(lambda x: x.lower(), sent.split())))\n",
        "        return result\n",
        "  \n",
        "    def fit(self, sequences):\n",
        "        self.fit_checker = False\n",
        "        token = self.preprocessing(sequences)\n",
        "        for sent in token:\n",
        "            for word in sent:\n",
        "                self.word_dict.setdefault(word, len(self.word_dict))\n",
        "        self.fit_checker = True\n",
        "  \n",
        "    def transform(self, sequences):\n",
        "        result = []\n",
        "        tokens = self.preprocessing(sequences)\n",
        "        if self.fit_checker:\n",
        "            for sent in tokens:\n",
        "                temp = []\n",
        "                for word in sent:\n",
        "                    temp.append(self.word_dict[word])\n",
        "                result.append(temp)\n",
        "            return result\n",
        "        else:\n",
        "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        result = self.transform(sequences)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.fit_checker = False\n",
        "  \n",
        "    def fit(self, sequences):\n",
        "        tokenized = self.tokenizer.fit_transform(sequences)\n",
        "        import math\n",
        "        n = len(self.tokenizer.word_dict)\n",
        "        self.idf = []\n",
        "        for i in range(n):\n",
        "            df = 0\n",
        "            for sent in tokenized:\n",
        "                if i in sent:\n",
        "                    df += 1\n",
        "            self.idf.append(math.log(n/(1+df)))\n",
        "\n",
        "        self.fit_checker = True\n",
        "    \n",
        "\n",
        "    def transform(self, sequences):\n",
        "        if self.fit_checker:\n",
        "            tokenized = self.tokenizer.transform(sequences)\n",
        "            self.tfidf_matrix = []\n",
        "            for sent in tokenized:\n",
        "                temp = []\n",
        "                for word in sent:\n",
        "                    tf = sent.count(word)\n",
        "                    temp.append(tf*self.idf[word])\n",
        "                self.tfidf_matrix.append(temp)\n",
        "            return self.tfidf_matrix\n",
        "        else:\n",
        "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        return self.transform(sequences)"
      ],
      "metadata": {
        "id": "72BWOMKbOOET"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}